
My project idea addresses issues related to students navigating across campus. This comes with a subset of problems such as indoor positioning system accuracy, easy to use user interface for both visually impaired and visually capable individuals. For a more descriptive analysis of the problems faced with navigating around campus, I asked AI the following: "What are the issues with navigating people around college campuses? What specific challenges do visually impaired people face with this problem?" Given this prompt, AI was able to inform me of several issues which were not immediately apparent in my previous analysis of the problem. The first important thing that AI pointed out was that temporary obstacles can make it difficult for individuals to navigate around campus, especially with a lack of signage informing people of the obstacles ahead (which must be available to the visually impaired as well). In addition to this, it can be difficult to get a sense for your general orientation on campus. Maybe there is a more nuanced situation in which you know you want to travel a certain direction (i.e. towards the city), but you don't actually know the current direction that you are facing relative to the world around you. Another important idea which I touched on was the inaccuracy of GPS indoors, and the lack of precision which can make it difficult to provide specific directions to small areas. 

What I know about this problem largely revolves around the GPS inaccuracy side of things. Several different technologies have emerged in order to attempt to solve this problem. The most common technologies revolve around the idea of "fingerprinting" an area. This involves taking note of  what environmental aspects are variable throughout the building, and making a map of these variables so that you can look back on this map and compare it with current data in order to get a relative sense of where you are. The data that is used for fingerprinting mostly comes from the user's cell phone. Cell phones are equipped with sensors and modules such as magnetometer data (x, y, z) [1], WIFI modules (used for determining and comparing signal strength between a predefined map and the user's current WIFI strength fingerprint) [2], camera/lidar sensors (used in conjunction with a technology called augmented reality (AR) which can provide information about the user's surrounding location through the camera) [3], and the general field called "sensor fusion" revolving around combining these technologies with a filter (such as a Kalman filter) [4]. When I asked AI what is done about the obstacle detection and orientation problems, it provided me with a few key insights. One of the most important technologies which isn't necessarily integrated into campus' and buildings yet is the idea of crowdsourced information. This is the idea that the users themselves can provide critical information regarding the location of obstacles throughout the building in order to prevent others from being harmed or obstructed by these objects. The orientation issues can be solved largely through the means that I described earlier regarding improving the accuracy of indoor positioning systems, but in addition to this a simple compass which describes to the user their general location is very helpful in assisting the user with determining their orientation. For the visually impaired, this could mean a blend of auditory and haptic feedback which allows the user to know when they are facing the correct direction to complete their task. All of these helpful tools will improve the user's life by relieving the stress associated with navigating about the campus, as well as helping visually impaired individuals avoid physical harm and eliminating the need of expensive external assistance devices.



